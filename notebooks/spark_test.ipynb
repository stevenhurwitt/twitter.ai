{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: twitter-api-key\n",
      "key: twitter-secret-key\n",
      "key: twitter-bearer\n",
      "key: twitter-access-token\n",
      "key: twitter-secret-access\n",
      "key: aws-access-key\n",
      "key: aws-secret-access-key\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "start = time.time()\n",
    "pp = pprint.PrettyPrinter(indent = 2)\n",
    "logger = logging.getLogger(\"reddit\")\n",
    "\n",
    "with open(\"../creds.json\", \"r\") as g:\n",
    "    conf = json.load(g)\n",
    "    for k in conf.keys():\n",
    "        print(\"key: {}\".format(k))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SecretsManager' object has no attribute 'getSecretValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m spark_host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspark-master\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[1;32m      8\u001b[0m extra_jar_list \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,io.delta:delta-core_2.12:2.2.0,org.postgresql:postgresql:42.5.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m aws_client \u001b[39m=\u001b[39m secretmanager\u001b[39m.\u001b[39;49mgetSecretValue(\u001b[39m\"\u001b[39m\u001b[39mAWS_ACCESS_KEY_ID\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m aws_secret \u001b[39m=\u001b[39m secretmanager\u001b[39m.\u001b[39mgetSecretValue(\u001b[39m\"\u001b[39m\u001b[39mAWS_SECRET_ACCESS_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m spark \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[1;32m     13\u001b[0m                 \u001b[39m.\u001b[39mmaster(\u001b[39m\"\u001b[39m\u001b[39mspark://\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:7077\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(spark_host)) \\\n\u001b[1;32m     14\u001b[0m                 \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.scheduler.mode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFAIR\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[39m.\u001b[39menableHiveSupport() \\\n\u001b[1;32m     34\u001b[0m                 \u001b[39m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m~/Documents/twitter.ai/twitter.ai/lib/python3.10/site-packages/botocore/client.py:876\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[39mif\u001b[39;00m event_response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     \u001b[39mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 876\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    877\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SecretsManager' object has no attribute 'getSecretValue'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pyspark\n",
    "\n",
    "secretmanager = boto3.client(\"secretsmanager\")\n",
    "\n",
    "subreddit = \"technology\"\n",
    "spark_host = \"spark-master\" \n",
    "extra_jar_list = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,io.delta:delta-core_2.12:2.2.0,org.postgresql:postgresql:42.5.0\"\n",
    "aws_client = secretmanager.getSecretValue(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret = secretmanager.getSecretValue(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
    "                .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "                .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
    "                .config(\"spark.executor.memory\", \"2048m\") \\\n",
    "                .config(\"spark.executor.cores\", \"2\") \\\n",
    "                .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "                .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "                .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "                .config('spark.hadoop.fs.s3a.buffer.dir', '/opt/workspace/tmp/blocks') \\\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                .enableHiveSupport() \\\n",
    "                .getOrCreate()\n",
    "\n",
    "logger.info(\"created spark session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter.ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55254a09b27e21896d77b069f7c5daccd61a03baf7db689fe62bada468b7bdce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
