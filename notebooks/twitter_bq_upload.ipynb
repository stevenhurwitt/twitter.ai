{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter data bigquery upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pprint\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import google.auth\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "logging.basicConfig(filename = 'twitter_bq_upload.log', level = logging.DEBUG)\n",
    "base = '/media/steven/big_boi/twitter.ai'\n",
    "data_dir = os.path.join(base, 'data')\n",
    "log_dir = os.path.join(base, 'logs')\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "#init bq client\n",
    "creds_fname = '/media/steven/big_boi/creds_google.json'\n",
    "client = bigquery.Client.from_service_account_json(creds_fname)\n",
    "bqstorageclient = bigquery_storage.BigQueryStorageClient.from_service_account_json(creds_fname)\n",
    "logging.info('initialized bigquery client.')\n",
    "print('imported modules successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data in bq table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_read_table():\n",
    "\n",
    "    #read current bq table\n",
    "    QUERY = \"SELECT * FROM `symbolic-bit-277217.basement_dude_tweets.tweets_master`;\"\n",
    "    query_job = client.query(QUERY)\n",
    "    results = client.query(QUERY).result()\n",
    "    bq_output = results.to_dataframe()\n",
    "    return(bq_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get new data to upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove date column b/c of difference b/t bigquery and dataframe date types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df(A, B):\n",
    "    to_upload = pd.concat([A.drop(columns = 'date'), B.drop(columns = 'date')]).drop_duplicates(keep = False)\n",
    "    to_upload = pd.concat([to_upload, A.date], axis = 1, join = 'inner')\n",
    "    to_upload = to_upload[['text', 'user', 'date', 'fav_count', 'retweet_count', 'retweet']]\n",
    "    to_upload.reset_index(drop = True, inplace = True)\n",
    "    to_upload.to_csv('master_data_upload.csv', index = False)\n",
    "\n",
    "    output = 'adding {} records to bq db.'.format(to_upload.shape[0])\n",
    "    print(output)\n",
    "    logging.info(output)\n",
    "    return(to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add date column back in from master .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload dataframe to bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up job to upload, see docs for WRITE_APPEND vs. WRITE_TRUNCATE vs. WRITE_EMPTY https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#bigquery_load_table_partitioned-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_upload():\n",
    "    #set table ref\n",
    "    table_ref = client.dataset('basement_dude_tweets').table('tweets_master')\n",
    "\n",
    "    #create job_config\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "    job_config.skip_leading_rows = 1\n",
    "\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.auto_detect = True\n",
    "    job_config.allow_jagged_rows = True\n",
    "    job_config.allow_quoted_newlines = True\n",
    "\n",
    "    #upload local file to bq\n",
    "    filename = os.path.join(data_dir, 'master_data_upload.csv')\n",
    "    with open(filename, \"rb\") as source_file:\n",
    "        load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    try:\n",
    "        load_job.result()  # Waits for table load to complete.\n",
    "        print(\"Job finished.\")\n",
    "\n",
    "        destination_table = client.get_table(table_ref)\n",
    "        output = \"Loaded {} rows.\".format(destination_table.num_rows)\n",
    "        print(output)\n",
    "        logging.info(output)\n",
    "    \n",
    "    except:\n",
    "        print('Job failed:')\n",
    "        for j in load_job.errors:\n",
    "            logging.error(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de-duplicate bq table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_dedupe():\n",
    "    QUERY = \"\"\"CREATE OR REPLACE TABLE `symbolic-bit-277217.basement_dude_tweets.tweets_master`\n",
    "            AS SELECT DISTINCT * FROM `symbolic-bit-277217.basement_dude_tweets.tweets_master`;\"\"\"\n",
    "    query_job = client.query(QUERY)\n",
    "    results = client.query(QUERY).result()\n",
    "    bq_output = results.to_dataframe()\n",
    "    \n",
    "    bq_output = bq_read_table()\n",
    "    print('de-duplicated bq table to {} results.'.format(bq_output.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        #read master data .csv\n",
    "        master = pd.read_csv('master_data_clean.csv')\n",
    "    except:\n",
    "        print('file import error.')\n",
    "\n",
    "    try:\n",
    "        #read bq table\n",
    "        bq_output = bq_read_table()\n",
    "    except:\n",
    "        print('bigquery read error.')\n",
    "\n",
    "    try:\n",
    "        #subset data to add\n",
    "        to_upload = subset_df(master, bq_output)\n",
    "    except:\n",
    "        print('data subset error.')\n",
    "        logging.warning('no new data to upload.')\n",
    "\n",
    "    try:\n",
    "        #upload new data to bq table\n",
    "        bq_upload()\n",
    "    except:\n",
    "        print('bigquery upload error.')\n",
    "        logging.warning('bq upload failed.')\n",
    "    try:\n",
    "        bq_dedupe()\n",
    "    except:\n",
    "        print('deduplication error.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 0 records to bq db.\n",
      "Starting job cc396734-6a09-4aa7-a25a-83065714b8d8\n",
      "Job finished.\n",
      "Loaded 14387 rows.\n",
      "de-duplicated bq table to 14387 results.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
