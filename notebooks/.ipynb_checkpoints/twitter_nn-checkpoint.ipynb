{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = '/media/steven/big_boi/twitter.ai'\n",
    "data_dir = os.path.join(base, 'data')\n",
    "log_dir = os.path.join(base, 'logs')\n",
    "\n",
    "os.chdir(base)\n",
    "print('set directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules successfully.\n",
      "using tf with dev: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')].\n",
      "imported modules successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "import logging.handlers\n",
    "import pprint\n",
    "import json\n",
    "import math\n",
    "import keras\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn as sk\n",
    "import google.auth\n",
    "import tensorflow as tf\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from twitter_bq_upload import bq_read_table\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "\n",
    "#init bq client\n",
    "creds_fname = '/media/steven/big_boi/creds_google.json'\n",
    "client = bigquery.Client.from_service_account_json(creds_fname)\n",
    "bqstorageclient = bigquery_storage.BigQueryStorageClient.from_service_account_json(creds_fname)\n",
    "logging.info('initialized bigquery client.')\n",
    "\n",
    "tf_dev = 'using tf with dev: {}.'.format(tf.config.list_physical_devices('GPU'))\n",
    "logging.info(tf_dev)\n",
    "print(tf_dev)\n",
    "print('imported modules successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data from bigquery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bq_read_table()\n",
    "tweets = list(data.text)\n",
    "np.random.shuffle(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove bad characters \n",
    "(\\n, \\\\, RT, https://..., http://, amp;, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet_list):\n",
    "    clean_tweet_list = []\n",
    "    bad_chars = ['\\n', '\\\\', 'amp;', 'RT ']\n",
    "    for t in tweet_list:\n",
    "        for b in bad_chars:\n",
    "            t = t.replace(b, '')\n",
    "        t = re.sub(r\"http\\S+\", '', t)\n",
    "        clean_tweet_list.append(t)\n",
    "    output = 'cleaned {} tweets.'.format(len(clean_tweet_list))\n",
    "    print(output)\n",
    "    #logging.info(output)\n",
    "    return(clean_tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned 15288 tweets.\n"
     ]
    }
   ],
   "source": [
    "clean_tweet = clean_tweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_tweets.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_tweet, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tweet_len = math.ceil(np.mean([len(c.split(' ')) for c in clean_tweet]))\n",
    "corp = \" \".join(clean_tweet)\n",
    "words = corp.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn tweets into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sequences(tokens, size):\n",
    "    length = size + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        seq = tokens[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "\n",
    "    return(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned 15288 tweets into 165957 sequences.\n"
     ]
    }
   ],
   "source": [
    "tweet_seq = tweet_sequences(words, avg_tweet_len)\n",
    "output = 'turned {} tweets into {} sequences.'.format(len(clean_tweet), len(tweet_seq))\n",
    "print(output)\n",
    "#logging.info(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize and split into X and y\n",
    "Tokenizing turns the words into numerical indices. \n",
    "y is the last word, X is the rest of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = nltk.tokenize.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_xy(sequence):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(sequence)\n",
    "    sequences = tokenizer.texts_to_sequences(sequence)\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(sequences)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    print('saved tokenizer')\n",
    " \n",
    "    X, y = padded[:,:-1], padded[:,-1]\n",
    "    y = keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    print('X has shape: {}'.format(X.shape))\n",
    "    print('y has shape: {}'.format(y.shape))\n",
    "    #logging.info('X has shape: {}'.format(X.shape))\n",
    "    #logging.info('y has shape: {}'.format(y.shape))\n",
    "    return(X, y, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved tokenizer\n",
      "X has shape: (165957, 29)\n",
      "y has shape: (165957, 17629)\n"
     ]
    }
   ],
   "source": [
    "X, y, s, v = tokenize_xy(tweet_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create the model architecture for our RNN. Inspiration comes from this blog post: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/. TF docs also have a text generation example but I'm unsure if it has been ported to Tensorflow 2.0 (which simplifies the library A LOT). Note that text generation can be done on the *character* or the *word* level. The linked example does it on the word level, which is what we want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X, y, seq_length, vocab_size):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, X.shape[1], input_length=seq_length))\n",
    "    model.add(keras.layers.LSTM(100, return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(100))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    #logging.info(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X, y, batch_size=128, epochs=250)\n",
    " \n",
    "\n",
    "    model.save('model.h5')\n",
    "\n",
    "    print('saved model.')\n",
    "    #logging.info('saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(X, y, s, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
