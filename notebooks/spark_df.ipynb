{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark dynamo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n",
      "read config & creds files.\n",
      "connected to boto3 clients.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import pprint\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "import botocore.session\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.column import *\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "print(\"imported modules.\")\n",
    "\n",
    "with open(\"./../creds.json\", \"r\") as g:\n",
    "    creds = json.load(g)\n",
    "    g.close()\n",
    "\n",
    "with open(\"./../config.yaml\", \"r\") as h:\n",
    "    config = yaml.safe_load(h)\n",
    "    h.close()\n",
    "\n",
    "print(\"read config & creds files.\")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "athena = boto3.client(\"athena\")\n",
    "glue = boto3.client(\"glue\")\n",
    "my_lambda = boto3.client(\"lambda\")\n",
    "dynamo = boto3.client(\"dynamodb\")\n",
    "print(\"connected to boto3 clients.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"twitter\") \\\n",
    "        .master(\"spark://{}:{}\".format(creds[\"spark_host\"], creds[\"spark_port\"])) \\\n",
    "        .config(\"spark.executor.memory\", \"2048m\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "        .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/\") \\\n",
    "        .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", \"/opt/workspace/tmp/events/\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .config(\"spark.jars.packages\", config[\"extra_jar_list\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_client\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.buffer.dir\", \"/opt/workspace/tmp/blocks\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    # index = 0\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    # sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
    "    print(\"imported modules, created spark.\")\n",
    "\n",
    "except Exception as f:\n",
    "    print(\"EXCEPTION: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = spark.read.format(\"delta\").option(\"header\", True).load(filepath)\n",
    "    # df.show()\n",
    "    print(\"read df.\")\n",
    "\n",
    "except Exception as g:\n",
    "    print(\"EXCEPTION: {}\".format(g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dynamodb client & resource.\n"
     ]
    }
   ],
   "source": [
    "dynamo_client = boto3.client(\"dynamodb\", region_name = \"us-east-2\")\n",
    "dynamo_resource = boto3.resource(\"dynamodb\", region_name = \"us-east-2\")\n",
    "print(\"created dynamodb client & resource.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table tweets has count 5552\n"
     ]
    }
   ],
   "source": [
    "tweets = dynamo_resource.Table(\"tweets\")\n",
    "response = tweets.scan()\n",
    "count = response[\"Count\"]\n",
    "print(\"table {} has count {}\".format(tweets.name, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response[\"Items\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fda94e2f7431d3d4e964823a72aa03a6377b2dcde79bd19077caf993a461d0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
